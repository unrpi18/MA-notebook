{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512, 91)\n",
      "(512, 512, 91)\n",
      "(512, 512, 91)\n",
      "(512, 512, 91)\n",
      "[0. 1.]\n",
      "[  0.  10.  15.  16.  17.  32.  44.  52.  54.  69.  70.  71.  72.  73.\n",
      "  74.  79.  86.  90.  91.  92. 118.]\n"
     ]
    }
   ],
   "source": [
    "import nibabel as nib\n",
    "from nibabel.processing import resample_from_to\n",
    "import numpy as np\n",
    "import torch\n",
    "ct_path = '/Users/keyi/Desktop/MA/HECKTOR/imagesTr/CHUM-001__CT.nii.gz'\n",
    "pet_path = '/Users/keyi/Desktop/MA/HECKTOR/imagesTr_resampled/CHUM-001__PT.nii.gz'\n",
    "label_path = '/Users/keyi/Desktop/MA/HECKTOR/labelsTr_resampled/CHUM-001.nii.gz'\n",
    "ana_path = '/Users/keyi/Desktop/MA/HECKTOR/totalsegmentator/CHUM-001__interpolated_ANASEG.nii.gz'\n",
    "ct = nib.load(ct_path)\n",
    "pet = nib.load(pet_path)\n",
    "label = nib.load(label_path)\n",
    "ana = nib.load(ana_path)\n",
    "print(ct.shape)\n",
    "print(pet.shape)\n",
    "print(label.shape)\n",
    "print(ana.shape)\n",
    "\n",
    "\n",
    "print(np.unique(label.get_fdata()))\n",
    "print(np.unique(ana.get_fdata()))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    SignalFillEmptyd,\n",
    "    Compose,\n",
    "    ConcatItemsd,\n",
    "    ToTensord,\n",
    "    RandShiftIntensityd,\n",
    "    CropForegroundd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    Transform,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    LambdaD,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandCropByPosNegLabeld,\n",
    ")\n",
    "keys = [\"image\", \"label\"]\n",
    "class ClipPercentiles(Transform):\n",
    "    def __init__(self, lower_percentile:float, upper_percentile:float) -> None:\n",
    "        super().__init__()\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def __call__(self, img:torch.Tensor):\n",
    "        lower_threshold =   np.quantile(img,self.lower_percentile)\n",
    "        upper_threshold = np.quantile(img,self.upper_percentile)\n",
    "        return torch.clamp(img, min=lower_threshold, max=upper_threshold)\n",
    "class ClipPercentaged(MapTransform):\n",
    "    def __init__(self, keys, lower_percentile: float, upper_percentile: float) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.clip_transform = ClipPercentiles(lower_percentile, upper_percentile)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.clip_transform(d[key])\n",
    "        return d\n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=keys),\n",
    "    EnsureChannelFirstd(keys=keys),\n",
    "    Orientationd(keys=keys, axcodes=\"RAS\"),\n",
    "    Spacingd(keys=keys, pixdim=[0.977, 0.977, 3.27], mode=(\"bilinear\", \"nearest\")),\n",
    "    CropForegroundd(keys=keys, source_key=\"image\", select_fn=lambda x: x > (-800 - 31.793294743021132)/41.282285307412266),\n",
    "    ClipPercentaged(\n",
    "            keys=[\"image\"],\n",
    "            lower_percentile=0.5,\n",
    "            upper_percentile=0.995\n",
    "        ),\n",
    "        \n",
    "        #Clip according to percentile in PET \n",
    "        #(include more varienty: Hypothesis: We should not clip off the spikes too much)\n",
    "        \n",
    "       \n",
    "        #Noramlize the CT image. This has been done according to the nnUnet dataset fingerprint style\n",
    "        NormalizeIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            subtrahend=31.793294743021132,\n",
    "            divisor=41.282285307412266\n",
    "        ),\n",
    "        #Normalize PET image. This has been calculated according to the nnUnet dataset fingerprint style\n",
    "        \n",
    "        \n",
    "    RandCropByPosNegLabeld(\n",
    "            keys=keys,\n",
    "            label_key=\"label\",\n",
    "            spatial_size=(64, 64, 64),\n",
    "            pos=1,\n",
    "            neg=0,\n",
    "            num_samples=50,\n",
    "            image_key=\"image\",\n",
    "            image_threshold=(-800 - 31.793294743021132)/41.282285307412266,\n",
    "            allow_smaller=False,\n",
    "            )\n",
    "])\n",
    "data = {\"image\": ct_path, \"label\": label_path}\n",
    "transformed = transforms(data)\n",
    "print(len(transformed))\n",
    "print(transformed[0][\"image\"].shape)\n",
    "print(transformed[0][\"label\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6236599999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "score = [0.6443,0.6546,0.6691,0.5980,0.5523]\n",
    "print(np.mean(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the difference is 3.2599999999999962%\n"
     ]
    }
   ],
   "source": [
    "a = 0.5257 - 0.4931\n",
    "print(f\"the difference is {a*100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvhci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
