{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    SignalFillEmptyd,\n",
    "    Compose,\n",
    "    ConcatItemsd,\n",
    "    ToTensord,\n",
    "    RandShiftIntensityd,\n",
    "    CropForegroundd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    Transform,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    LambdaD,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandCropByLabelClassesd,\n",
    ")\n",
    "\n",
    "ct_path = \"/Users/keyi/Desktop/MA/AutoPet_Anatomy/FDG-PET-CT-Lesions/PETCT_0af7ffe12a/08-12-2005-NA-PET-CT Ganzkoerper  primaer mit KM-96698/CTres.nii.gz\"  \n",
    "save_dir = \"output\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"ct_transform.nii.gz\")\n",
    "\n",
    "class ClipPercentiles(Transform):\n",
    "    def __init__(self, lower_percentile:float, upper_percentile:float) -> None:\n",
    "        super().__init__()\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def __call__(self, img:torch.Tensor):\n",
    "        lower_threshold =   np.quantile(img,self.lower_percentile)\n",
    "        upper_threshold = np.quantile(img,self.upper_percentile)\n",
    "        return torch.clamp(img, min=lower_threshold, max=upper_threshold)\n",
    "\n",
    "class ClipPercentaged(MapTransform):\n",
    "    def __init__(self, keys, lower_percentile: float, upper_percentile: float) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.clip_transform = ClipPercentiles(lower_percentile, upper_percentile)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.clip_transform(d[key])\n",
    "        return d\n",
    "        \n",
    "transforms = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    Spacingd(\n",
    "        keys=[\"image\"],\n",
    "        pixdim=(2.04, 2.04, 3.0), \n",
    "        mode=(\"bilinear\")\n",
    "    ),\n",
    "    ClipPercentaged(\n",
    "            keys=[\"image\"],\n",
    "            lower_percentile=0.5,\n",
    "            upper_percentile=0.995\n",
    "        ),\n",
    "    NormalizeIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            subtrahend=-79.55364831013831,\n",
    "            divisor=158.5583080580807\n",
    "        ),\n",
    "    RandGaussianNoised(keys=[\"image\"], std=0.1, prob=0.15),\n",
    "        RandGaussianSmoothd(\n",
    "            keys=[\"image\"],\n",
    "            sigma_x=(0.5, 1),\n",
    "            sigma_y=(0.5, 1),\n",
    "            sigma_z=(0.5, 1),\n",
    "            prob=0.2,\n",
    "        ),\n",
    "        # random flip along the spatial axis (all axes)\n",
    "        RandFlipd(keys=[\"image\"], \n",
    "                  spatial_axis=[0,1,2], \n",
    "                  prob=0.05\n",
    "                  ),\n",
    "        RandRotate90d(keys=[\"image\"], \n",
    "                      prob=0.05\n",
    "                      ),\n",
    "        RandShiftIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            offsets=0.15,\n",
    "            prob=0.1\n",
    "        ),\n",
    "        SignalFillEmptyd(\n",
    "            keys=[\"image\"]\n",
    "            ),\n",
    "])\n",
    "\n",
    "\n",
    "data = {\"image\": ct_path}\n",
    "transformed_data = transforms(data)\n",
    "ct_data = transformed_data[\"image\"]\n",
    "\n",
    "print(ct_data.shape)\n",
    "if isinstance(ct_data, torch.Tensor):\n",
    "    ct_data = ct_data.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "original_img = nib.load(ct_path)\n",
    "affine = original_img.affine\n",
    "ct_data = ct_data[0]\n",
    "print(f\"now ct_data.shape is {ct_data.shape}\")\n",
    "nib.save(nib.Nifti1Image(ct_data, affine), 'ct_transform_0af7ffe12a.nii.gz')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from monai.transforms import (\n",
    "    EnsureChannelFirstd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    SignalFillEmptyd,\n",
    "    Compose,\n",
    "    ConcatItemsd,\n",
    "    ToTensord,\n",
    "    RandShiftIntensityd,\n",
    "    CropForegroundd,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    Transform,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    LambdaD,\n",
    "    RandGaussianNoised,\n",
    "    RandGaussianSmoothd,\n",
    "    RandCropByLabelClassesd,\n",
    "    GaussianSmoothd,\n",
    "    ScaleIntensityd,\n",
    ")\n",
    "\n",
    "ct_path = \"/Users/keyi/Desktop/MA/AutoPet_Anatomy/FDG-PET-CT-Lesions/PETCT_0af7ffe12a/08-12-2005-NA-PET-CT Ganzkoerper  primaer mit KM-96698/CTres.nii.gz\"  \n",
    "save_dir = \"output\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, \"ct_transform.nii.gz\")\n",
    "\n",
    "class ClipPercentiles(Transform):\n",
    "    def __init__(self, lower_percentile:float, upper_percentile:float) -> None:\n",
    "        super().__init__()\n",
    "        self.lower_percentile = lower_percentile\n",
    "        self.upper_percentile = upper_percentile\n",
    "\n",
    "    def __call__(self, img:torch.Tensor):\n",
    "        lower_threshold =   np.quantile(img,self.lower_percentile)\n",
    "        upper_threshold = np.quantile(img,self.upper_percentile)\n",
    "        return torch.clamp(img, min=lower_threshold, max=upper_threshold)\n",
    "\n",
    "class ClipPercentaged(MapTransform):\n",
    "    def __init__(self, keys, lower_percentile: float, upper_percentile: float) -> None:\n",
    "        super().__init__(keys)\n",
    "        self.clip_transform = ClipPercentiles(lower_percentile, upper_percentile)\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            d[key] = self.clip_transform(d[key])\n",
    "        return d\n",
    "        \n",
    "MONAI_BUNDLE_TRANSFORMS = Compose([\n",
    "    LoadImaged(keys=[\"image\"]),\n",
    "    EnsureChannelFirstd(keys=[\"image\"]),\n",
    "    Orientationd(keys=[\"image\"], axcodes=\"RAS\"),\n",
    "    Spacingd(\n",
    "        keys=[\"image\"],\n",
    "        pixdim=(1.5,1.5,1.5), \n",
    "        mode=(\"bilinear\")\n",
    "    ),\n",
    "    NormalizeIntensityd(\n",
    "            keys=[\"image\"],\n",
    "            nonzero=True,\n",
    "        ),\n",
    "    GaussianSmoothd(\n",
    "        keys=[\"image\"],\n",
    "        sigma=0.4\n",
    "    ),\n",
    "    ScaleIntensityd(\n",
    "        keys=[\"image\"],\n",
    "        minv=-1.0,\n",
    "        maxv=1.0\n",
    "    ),\n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "data = {\"image\": ct_path}\n",
    "transformed_data = MONAI_BUNDLE_TRANSFORMS(data)\n",
    "ct_data = transformed_data[\"image\"]\n",
    "\n",
    "print(ct_data.shape)\n",
    "if isinstance(ct_data, torch.Tensor):\n",
    "    ct_data = ct_data.cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "original_img = nib.load(ct_path)\n",
    "affine = original_img.affine\n",
    "ct_data = ct_data[0]\n",
    "print(f\"now ct_data.shape is {ct_data.shape}\")\n",
    "nib.save(nib.Nifti1Image(ct_data, affine), 'ct_monai_transform_0af7ffe12a.nii.gz')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvhci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
