{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[256], expected input with shape [*, 256], but got input of size[1, 27, 64]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(split_x1, split_x2):\n\u001b[0;32m--> 102\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mcross_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    103\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, C, D, H, W)\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 75\u001b[0m, in \u001b[0;36mShiftedWindowCrossAttention.forward\u001b[0;34m(self, q_feat, k_feat)\u001b[0m\n\u001b[1;32m     72\u001b[0m     q_window \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(q_window, torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, device\u001b[38;5;241m=\u001b[39mq_window\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[1;32m     73\u001b[0m     k_window \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlinear(k_window, torch\u001b[38;5;241m.\u001b[39meye(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, device\u001b[38;5;241m=\u001b[39mk_window\u001b[38;5;241m.\u001b[39mdevice))\n\u001b[0;32m---> 75\u001b[0m     out_window \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_window\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_window\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Process each window separately\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out_window\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))  \u001b[38;5;66;03m# Transpose back\u001b[39;00m\n\u001b[1;32m     78\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# (B, num_windows, window_size, D*H*W)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/monai/networks/blocks/transformerblock.py:94\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, context: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 94\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_cross_attention:\n\u001b[1;32m     96\u001b[0m         x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_cross_attn(x), context\u001b[38;5;241m=\u001b[39mcontext)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/modules/normalization.py:202\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/cvhci/lib/python3.10/site-packages/torch/nn/functional.py:2576\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2573\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2574\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2575\u001b[0m     )\n\u001b[0;32m-> 2576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[256], expected input with shape [*, 256], but got input of size[1, 27, 64]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from monai.networks.blocks import TransformerBlock\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from monai.networks.blocks import TransformerBlock\n",
    "\n",
    "class ShiftedWindowCrossAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=8, window_size=4, shift_size=2, dropout_rate=0.1):\n",
    "        \"\"\"\n",
    "        Shifted Window Cross Attention on Channel Dimension\n",
    "        Args:\n",
    "            in_channels: Input channels\n",
    "            num_heads: Number of attention heads\n",
    "            window_size: Size of each window in channel dimension\n",
    "            shift_size: Size to shift the windows\n",
    "            dropout_rate: Dropout rate\n",
    "        \"\"\"\n",
    "        super(ShiftedWindowCrossAttention, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        \n",
    "        # Change hidden_size to match the flattened spatial dimensions\n",
    "        self.attn = TransformerBlock(\n",
    "            hidden_size=in_channels,  # Changed back to in_channels\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=in_channels * 4,\n",
    "            qkv_bias=False,\n",
    "            with_cross_attention=True,\n",
    "            use_flash_attention=True,\n",
    "            dropout_rate=dropout_rate,\n",
    "        )\n",
    "\n",
    "    def forward(self, q_feat, k_feat):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q_feat: Features from modality 1 (B, C, D, H, W)\n",
    "            k_feat: Features from modality 2 (B, C, D, H, W)\n",
    "        Returns:\n",
    "            Output features (B, C, D, H, W)\n",
    "        \"\"\"\n",
    "        B, C, D, H, W = q_feat.shape\n",
    "        \n",
    "        # Create windows along channel dimension\n",
    "        q_windows = q_feat.view(B, -1, self.window_size, D, H, W)  # (B, num_windows, window_size, D, H, W)\n",
    "        k_windows = k_feat.view(B, -1, self.window_size, D, H, W)\n",
    "        \n",
    "        # Apply cyclic shift along channel dimension\n",
    "        q_shifted = torch.roll(q_windows, shifts=-self.shift_size, dims=2)\n",
    "        k_shifted = torch.roll(k_windows, shifts=-self.shift_size, dims=2)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        q = q_shifted.flatten(3)  # (B, num_windows, window_size, D*H*W)\n",
    "        k = k_shifted.flatten(3)\n",
    "        \n",
    "        # Process each window\n",
    "        outputs = []\n",
    "        for i in range(q.size(1)):\n",
    "            # Reshape to match TransformerBlock expectations\n",
    "            q_window = q[:, i].transpose(1, 2)  # (B, D*H*W, window_size)\n",
    "            k_window = k[:, i].transpose(1, 2)  # (B, D*H*W, window_size)\n",
    "            \n",
    "            # Project to match hidden_size\n",
    "            q_window = F.linear(q_window, torch.eye(self.window_size, device=q_window.device))\n",
    "            k_window = F.linear(k_window, torch.eye(self.window_size, device=k_window.device))\n",
    "            \n",
    "            out_window = self.attn(q_window, k_window)  # Process each window separately\n",
    "            outputs.append(out_window.transpose(1, 2))  # Transpose back\n",
    "        \n",
    "        out = torch.stack(outputs, dim=1)  # (B, num_windows, window_size, D*H*W)\n",
    "        \n",
    "        # Reshape back and reverse shift\n",
    "        out = out.view(B, -1, self.window_size, D, H, W)\n",
    "        out = torch.roll(out, shifts=self.shift_size, dims=2)\n",
    "        out = out.reshape(B, C, D, H, W)\n",
    "        \n",
    "        return out\n",
    "\n",
    "channels = 1024\n",
    "input = torch.randn(1,1024,3,3,3)\n",
    "groups = 4\n",
    "\n",
    "split_x1 = torch.chunk(input, groups, dim=1) # [B, C/groups, D, H, W]\n",
    "split_x2 = torch.chunk(input, groups, dim=1)\n",
    "cross_attn = ShiftedWindowCrossAttention(\n",
    "    in_channels=1024 // groups, \n",
    "    num_heads=2, \n",
    "    window_size=64,  # Window size in channel dimension\n",
    "    shift_size=32,   # Shift size\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "outputs = []\n",
    "for q, k in zip(split_x1, split_x2):\n",
    "    outputs.append(cross_attn(q, k))\n",
    "output = torch.cat(outputs, dim=1)  # (B, C, D, H, W)\n",
    "print(output.shape)  # 应为 (1, 1024, 3, 3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keyi/Desktop/test_src/fold_0/PRED/CHUM-007_PRED.nii.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "check_dir = '/Users/keyi/Desktop/test_src/fold_0'\n",
    "save_dir = \"/\".join(check_dir.split(\"/\")) + \"/PRED\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "path = os.path.join(save_dir, \"CHUM-007_PRED.nii.gz\")\n",
    "print(path)\n",
    "\n",
    "def get_unique_filename(base_path: str, filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique filename by adding a counter if the file already exists.\n",
    "    \n",
    "    Args:\n",
    "        base_path: Directory path where the file will be saved\n",
    "        filename: Original filename with extension\n",
    "        \n",
    "    Returns:\n",
    "        str: A unique filename that doesn't exist in the base_path\n",
    "        \n",
    "    Example:\n",
    "        >>> get_unique_filename('/path/to/dir', 'image.nii.gz')\n",
    "        'image.nii.gz'  # If file doesn't exist\n",
    "        'image_1.nii.gz'  # If 'image.nii.gz' exists\n",
    "    \"\"\"\n",
    "    file_path = Path(base_path) / filename\n",
    "    \n",
    "    # Return original filename if it doesn't exist\n",
    "    if not file_path.exists():\n",
    "        return filename\n",
    "        \n",
    "    # Split filename into name and extension\n",
    "    stem = file_path.stem  # Gets filename without extension\n",
    "    suffix = file_path.suffix  # Gets extension including the dot\n",
    "    \n",
    "    # Handle double extensions (e.g., .nii.gz)\n",
    "    if stem.endswith('.nii') and suffix == '.gz':\n",
    "        stem = stem[:-4]  # Remove '.nii'\n",
    "        suffix = '.nii.gz'\n",
    "    \n",
    "    # Try incremental numbers until a unique filename is found\n",
    "    counter = 1\n",
    "    while (Path(base_path) / f\"{stem}_{counter}{suffix}\").exists():\n",
    "        counter += 1\n",
    "        \n",
    "    return f\"{stem}_{counter}{suffix}\"\n",
    "save_dir = Path('/Users/keyi/Desktop/test_src/fold_0') / \"PRED\"\n",
    "filename = get_unique_filename(save_dir, \"PRED_CHUM-007.nii.gz\")\n",
    "with open(os.path.join(save_dir, filename), 'w') as f:\n",
    "    f.write(str(\"ok\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PETCT_0dbf2c2731\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "src = \"FDG-PET-CT-Lesions/PETCT_0dbf2c2731/10-27-2005-NA-PET-CT Ganzkoerper  primaer mit KM-07954/CTres.nii.gz\"\n",
    "person_uid = src.split(\"/\")[1]\n",
    "print(person_uid)\n",
    "json_dir = '/Users/keyi/Desktop/DL_template/_assets/split_json/AutoPET_Cluster'\n",
    "for file in os.listdir(json_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(json_dir, file), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            for item in data[\"VALIDATION\"]:\n",
    "                person_uid = item[\"CTRES\"].split(\"/\")[0]\n",
    "                item[\"PERSON_UID\"] = person_uid\n",
    "        \n",
    "            \n",
    "        with open(os.path.join(json_dir, file), 'w') as f:\n",
    "            json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kit/anthropomatik/ew2572/trained_old/2_channels/baseline/mednext/fold_0\n"
     ]
    }
   ],
   "source": [
    "save_dir = Path('/home/kit/anthropomatik/ew2572/trained_old/2_channels/baseline/mednext/fold_0/fold=0-epoch=299-step=26700-Dice=0.9258.ckpt').parent\n",
    "print(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0061200000000000004\n",
      "torch.Size([400, 400, 284])\n",
      "[-1.       2.03642  2.03642  3.       1.       1.       1.       1.     ]\n",
      "0.0124848\n",
      "0.0061200000000000004\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "volume = np.prod([2.04, 2.04, 3.0][1:4]) / 1000\n",
    "print(volume)\n",
    "\n",
    "import monai.transforms as mt\n",
    "pet_file = '/Users/keyi/Desktop/MA/AutoPet_Anatomy/FDG-PET-CT-Lesions/08-10-2001-NA-PET-CT Ganzkoerper  primaer mit KM-23662/SUV.nii.gz'\n",
    "reference = mt.LoadImage()(pet_file)\n",
    "print(reference.shape)\n",
    "print(reference.meta[\"pixdim\"])\n",
    "print(np.prod([2.04, 2.04, 3.0])/1000)\n",
    "# volume = np.prod(reference.meta[\"pixdim\"][1:4]) / 1000\n",
    "print(volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 1024, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "class ShiftedGroupAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=4, groups=4):\n",
    "        super().__init__()\n",
    "        self.groups = groups\n",
    "        self.channels_per_group = in_channels // groups\n",
    "        \n",
    "        self.window_attention = TransformerBlock(\n",
    "            hidden_size=self.channels_per_group,\n",
    "            num_heads=num_heads,\n",
    "            mlp_dim=self.channels_per_group * 4,\n",
    "            qkv_bias=True,\n",
    "            with_cross_attention=True,\n",
    "            use_flash_attention=True,\n",
    "            dropout_rate=0.1,\n",
    "        )\n",
    "        \n",
    "        self.shift_sizes = [1, 2]\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        B, C, D, H, W = x1.shape\n",
    "        outputs = []\n",
    "        \n",
    "  \n",
    "        x1_groups = torch.chunk(x1, self.groups, dim=1)  # [B, C/groups, D, H, W]\n",
    "        x2_groups = torch.chunk(x2, self.groups, dim=1)\n",
    "        \n",
    "    \n",
    "        for g_idx in range(self.groups):\n",
    "            q = x1_groups[g_idx].flatten(2).transpose(1, 2)  # [B, D*H*W, C/groups]\n",
    "            k = x2_groups[g_idx].flatten(2).transpose(1, 2)\n",
    "            out = self.window_attention(q, k).transpose(1, 2).view(B, -1, D, H, W)\n",
    "            outputs.append(out)\n",
    "        \n",
    "\n",
    "        for shift_size in self.shift_sizes:\n",
    "  \n",
    "            x1_shift = torch.roll(x1, shifts=self.channels_per_group * shift_size, dims=1)\n",
    "            x2_shift = torch.roll(x2, shifts=self.channels_per_group * shift_size, dims=1)\n",
    "            \n",
    "            x1_shift_groups = torch.chunk(x1_shift, self.groups, dim=1)\n",
    "            x2_shift_groups = torch.chunk(x2_shift, self.groups, dim=1)\n",
    "            \n",
    "            for g_idx in range(self.groups):\n",
    "                q = x1_shift_groups[g_idx].flatten(2).transpose(1, 2)\n",
    "                k = x2_shift_groups[g_idx].flatten(2).transpose(1, 2)\n",
    "                out = self.window_attention(q, k).transpose(1, 2).view(B, -1, D, H, W)\n",
    "                outputs.append(out)\n",
    "        \n",
    "    \n",
    "        outputs = torch.stack([torch.cat(outputs[i:i+self.groups], dim=1) \n",
    "                             for i in range(0, len(outputs), self.groups)], dim=0)\n",
    "        return outputs.mean(0)  # [B, C, D, H, W]\n",
    "\n",
    "x1 = torch.randn(12, 1024, 3, 3, 3)  \n",
    "x2 = torch.randn(12, 1024, 3, 3, 3)\n",
    "out = ShiftedGroupAttention(1024, 4, 4)(x1, x2)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvhci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
